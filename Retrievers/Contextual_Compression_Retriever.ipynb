{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \\\n",
        "  langchain \\\n",
        "  langchain-core \\\n",
        "  langchain-community \\\n",
        "  langchain-openai \\\n",
        "  faiss-cpu \\\n",
        "  openai \\\n",
        "  tiktoken\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikPwwy3rCXnA",
        "outputId": "efd53435-94f4-4bce-af07-92c31aa5808a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.4)\n",
            "Collecting langchain\n",
            "  Downloading langchain-1.2.7-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.2.7)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-1.1.7-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.15.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Collecting langgraph<1.1.0,>=1.0.7 (from langchain)\n",
            "  Downloading langgraph-1.0.7-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.6.4)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.13.0)\n",
            "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n",
            "  Downloading langchain_classic-1.0.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.45)\n",
            "Collecting requests<3.0.0,>=2.32.5 (from langchain-community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.3)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Collecting langchain-text-splitters<2.0.0,>=1.1.0 (from langchain-classic<2.0.0,>=1.0.0->langchain-community)\n",
            "  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain) (4.0.0)\n",
            "Collecting langgraph-prebuilt<1.1.0,>=1.0.7 (from langgraph<1.1.0,>=1.0.7->langchain)\n",
            "  Downloading langgraph_prebuilt-1.0.7-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain) (0.3.3)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain) (3.6.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.7->langchain) (1.12.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain-1.2.7-py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.8/108.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-1.1.7-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.13.2-cp310-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (23.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_classic-1.0.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-1.0.7-py3-none-any.whl (157 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.4/157.4 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n",
            "Downloading langgraph_prebuilt-1.0.7-py3-none-any.whl (35 kB)\n",
            "Downloading marshmallow-3.26.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: requests, mypy-extensions, marshmallow, faiss-cpu, typing-inspect, dataclasses-json, langchain-text-splitters, langchain-openai, langgraph-prebuilt, langchain-classic, langgraph, langchain-community, langchain\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.4\n",
            "    Uninstalling requests-2.32.4:\n",
            "      Successfully uninstalled requests-2.32.4\n",
            "  Attempting uninstall: langgraph-prebuilt\n",
            "    Found existing installation: langgraph-prebuilt 1.0.6\n",
            "    Uninstalling langgraph-prebuilt-1.0.6:\n",
            "      Successfully uninstalled langgraph-prebuilt-1.0.6\n",
            "  Attempting uninstall: langgraph\n",
            "    Found existing installation: langgraph 1.0.6\n",
            "    Uninstalling langgraph-1.0.6:\n",
            "      Successfully uninstalled langgraph-1.0.6\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 1.2.4\n",
            "    Uninstalling langchain-1.2.4:\n",
            "      Successfully uninstalled langchain-1.2.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 faiss-cpu-1.13.2 langchain-1.2.7 langchain-classic-1.0.1 langchain-community-0.4.1 langchain-openai-1.1.7 langchain-text-splitters-1.1.0 langgraph-1.0.7 langgraph-prebuilt-1.0.7 marshmallow-3.26.2 mypy-extensions-1.1.0 requests-2.32.5 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nLkSx5ooAuDx"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain_core.documents import Document"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "\n",
        "documents = [\n",
        "    Document(\n",
        "        page_content=\"\"\"\n",
        "        Contextual compression is a technique used in retrieval-augmented\n",
        "        generation systems. While many documents contain useful information,\n",
        "        they often also include background explanations, historical notes,\n",
        "        or marketing language. ContextualCompressionRetriever uses an LLM\n",
        "        to extract only the parts of a document that are relevant to a user query.\n",
        "        \"\"\",\n",
        "        metadata={\"source\": \"doc1\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"\"\"\n",
        "        FAISS is a popular vector database developed by Facebook AI Research.\n",
        "        It supports efficient similarity search and clustering of dense vectors.\n",
        "        In practical RAG pipelines, FAISS is often paired with retrievers.\n",
        "        However, the retrieved documents may still contain unnecessary sections,\n",
        "        which is where contextual compression becomes useful.\n",
        "        \"\"\",\n",
        "        metadata={\"source\": \"doc2\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"\"\"\n",
        "        Traditional retrievers return full documents or chunks even if only\n",
        "        a single sentence is relevant. Contextual compression solves this by\n",
        "        selectively extracting relevant passages before sending them to the LLM.\n",
        "        This helps reduce token usage and improve answer quality.\n",
        "        \"\"\",\n",
        "        metadata={\"source\": \"doc3\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"\"\"\n",
        "        In many enterprise knowledge bases, documents are written by different\n",
        "        teams and include inconsistent formatting, redundant explanations,\n",
        "        and off-topic sections. ContextualCompressionRetriever helps clean\n",
        "        these documents dynamically at query time.\n",
        "        \"\"\",\n",
        "        metadata={\"source\": \"doc4\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"\"\"\n",
        "        Large language models can act as intelligent filters. When used as\n",
        "        document compressors, they analyze retrieved text and keep only\n",
        "        information that directly answers the user question.\n",
        "        The rest of the content is discarded.\n",
        "        \"\"\",\n",
        "        metadata={\"source\": \"doc5\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"\"\"\n",
        "        Compression retrievers are especially helpful when documents are long\n",
        "        or when chunking alone is not sufficient. Instead of relying only on\n",
        "        embedding similarity, an LLM evaluates semantic relevance in context.\n",
        "        \"\"\",\n",
        "        metadata={\"source\": \"doc6\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"\"\"\n",
        "        Contextual compression introduces additional LLM calls, which can\n",
        "        increase latency and cost. For this reason, it is typically used\n",
        "        after an initial retrieval step rather than as a standalone retriever.\n",
        "        \"\"\",\n",
        "        metadata={\"source\": \"doc7\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"\"\"\n",
        "        When users ask focused questions, such as asking for a definition\n",
        "        or a specific mechanism, contextual compression can remove\n",
        "        introductory sections and examples that are not directly relevant.\n",
        "        \"\"\",\n",
        "        metadata={\"source\": \"doc8\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"\"\"\n",
        "        In contrast to reranking, which orders documents by relevance,\n",
        "        contextual compression modifies document content itself by\n",
        "        extracting only the most useful spans of text.\n",
        "        \"\"\",\n",
        "        metadata={\"source\": \"doc9\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"\"\"\n",
        "        A common pattern in RAG systems is: retrieve broadly, then compress.\n",
        "        This allows the system to maintain high recall while still providing\n",
        "        concise, high-signal context to the language model.\n",
        "        \"\"\",\n",
        "        metadata={\"source\": \"doc10\"}\n",
        "    ),\n",
        "]\n"
      ],
      "metadata": {
        "id": "DLn2QrkSCv1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model=OpenAIEmbeddings()\n",
        "vectorstore=FAISS.from_documents(documents,embedding_model)\n"
      ],
      "metadata": {
        "id": "EE2h-swmDCoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_retriever=vectorstore.as_retriever(search_kwargs={\"k\":5})\n"
      ],
      "metadata": {
        "id": "W3Rn0KSDDKcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set up the compressor using an LLM\n",
        "llm=ChatOpenAI(temperature=0)\n",
        "compressor=LLMChainExtractor.from_llm(llm)"
      ],
      "metadata": {
        "id": "aCIKyS0mDTLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compression_retriever=ContextualCompressionRetriever(\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=base_retriever\n",
        ")"
      ],
      "metadata": {
        "id": "Q74uUxk8DY26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Query the retriever\n",
        "query=\"What is photosynthesis?\"\n",
        "compressed_docs=compression_retriever.invoke(query)"
      ],
      "metadata": {
        "id": "sh-qzEYfDc2V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}